{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG (Retrieval-Augmented Generation) System using Llama 3 405B Model\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1o0X6SXgwjkZ12aGPra7_KYMfPKfZhHCD?usp=sharing#scrollTo=ruXVpm7Op6bL)\n",
        "\n",
        "####Enhancing language model outputs with relevant external knowledge\n",
        "####Combining the power of large language models with dynamic information retrieval"
      ],
      "metadata": {
        "id": "ruXVpm7Op6bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required dependencies\n",
        "\n",
        "### Note: In a Jupyter notebook, you would use !pip install or %pip install\n"
      ],
      "metadata": {
        "id": "2GUsOHtCpjn8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htke3C6BJ60t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a896f1ca-349b-4265-fd59-70280a944d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.17-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m725.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.37.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-community\n",
            "  Downloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3.0,>=0.2.20 (from langchain-openai)\n",
            "  Downloading langchain_core-0.2.23-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.93-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.20->langchain-openai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.20->langchain-openai)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: orjson, mypy-extensions, marshmallow, jsonpointer, h11, faiss-cpu, typing-inspect, tiktoken, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.8.0.post1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.11 langchain-community-0.2.10 langchain-core-0.2.23 langchain-openai-0.1.17 langchain-text-splitters-0.2.2 langsmith-0.1.93 marshmallow-3.21.3 mypy-extensions-1.0.0 openai-1.37.0 orjson-3.10.6 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-openai faiss-cpu langchain openai requests numpy tiktoken langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import necessary libraries\n"
      ],
      "metadata": {
        "id": "aykr9KRWpB3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wHLBlaREe_Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the language model (LLM)\n",
        "### We're using the Llama 3 405B model hosted on Fireworks.ai"
      ],
      "metadata": {
        "id": "uklMfmdsrO_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\",\n",
        "    openai_api_key=userdata.get(\"FIREWORKS_API_KEY\"),\n",
        "    openai_api_base=\"https://api.fireworks.ai/inference/v1\"\n",
        ")"
      ],
      "metadata": {
        "id": "6yik2PCFpAiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create embeddings for the documents\n",
        "### Note: We're using OpenAI's embeddings here. You might want to use a different embedding model.\n"
      ],
      "metadata": {
        "id": "GUwTZNwAoYms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=userdata.get(\"OPENAI_API_KEY\"))\n"
      ],
      "metadata": {
        "id": "G1AZxiLrk4r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare your document collection using WebBaseLoader\n",
        "### We're using WebBaseLoader to load content from specified URLs\n"
      ],
      "metadata": {
        "id": "bTVNISEwo2SU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://www.theverge.com/2024/7/23/24204055/meta-ai-llama-3-1-open-source-assistant-openai-chatgpt\",\n",
        "    # Add more URLs as needed\n",
        "]\n",
        "loader = WebBaseLoader(urls)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "QLskT7IelTh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the documents into chunks\n",
        "### This makes it easier to process and retrieve relevant information"
      ],
      "metadata": {
        "id": "L_WdzAJVoh5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "nIE3VaLEmAC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create embeddings for the documents and store them in a FAISS vector database\n"
      ],
      "metadata": {
        "id": "teZYIFsFolN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "G1aO4JIgmCiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine retrieval and generation\n",
        "#### This sets up the RAG system, combining the FAISS database for retrieval\n",
        "#### with the Llama 3 405B model for generation"
      ],
      "metadata": {
        "id": "ILi3pnQdoomW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    return_source_documents=True,\n",
        ")"
      ],
      "metadata": {
        "id": "x6P-pryWnYGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the RAG system with example queries\n",
        "#### You can modify these queries or add more as needed"
      ],
      "metadata": {
        "id": "Qb1iTqGVos_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Query 1"
      ],
      "metadata": {
        "id": "0LbFZRMtsEZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Meta's new Llama 3.1 model outperforms which models?\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "print(result['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VajRyJTbncg-",
        "outputId": "574548e1-30d6-44ef-ac71-9ddb6f71e5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the text, Meta's Llama 3.1 model outperforms OpenAI's GPT-4o and Anthropic's Claude 3.5 Sonnet on certain benchmarks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Query 2"
      ],
      "metadata": {
        "id": "PSJ6RCeNsGnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"give me top features of Llama 3.1 model\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "print(result['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsKyUEh5r_te",
        "outputId": "b9b226ad-803e-4bab-9640-c71c2f857f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the text, here are some of the top features of the Llama 3.1 model:\n",
            "\n",
            "1. Complex problem-solving: Llama 3.1 is capable of integrating with a search engine API to \"retrieve information from the internet based on a complex query and call multiple tools in succession in order to complete your tasks\".\n",
            "\n",
            "2. Advanced natural language processing: It can understand and respond to complex queries and is capable of generating human-like text.\n",
            "\n",
            "3. Image generation: Meta AI, powered by Llama 3.1, includes a new feature called \"Imagine Me\" that can scan a face through a phone's camera and then let users insert their likeness into images it generates.\n",
            "\n",
            "4. Multilingual support: Llama 3.1 will be updated to support new languages, including French, German, Hindi, Italian, and Spanish.\n",
            "\n",
            "5. Availability across multiple platforms: Llama 3.1 will be first accessible through WhatsApp and the Meta AI website in the US, followed by Instagram and Facebook in the coming weeks, and also on the Quest headset in the coming weeks.\n",
            "\n",
            "6. Free usage: Meta is releasing Llama 3.1 as an open-source model, free for developers to use and tune to their liking. \n",
            "\n",
            "7. Performance: Meta claims that Llama 3.1 outperforms GPT-4o and Anthropic’s Claude 3.5 Sonnet on several benchmarks.\n"
          ]
        }
      ]
    }
  ]
}