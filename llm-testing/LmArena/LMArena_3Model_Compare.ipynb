{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1iHtEzN_Bb9nfN4TJL_H46sy7b_PoZO-w#scrollTo=lVkgCDANH0u_)\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- No coding experience required\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H58Uz4-_JyCB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVkgCDANH0u_"
      },
      "source": [
        "# Top 3: Text [LMArena](https://lmarena.ai/leaderboard/text) Model Comparison\n",
        "\n",
        "This notebook compares three top-performing models via OpenRouter:\n",
        "\n",
        "- `google/gemini-2.5-pro`\n",
        "- `openai/gpt-5`\n",
        "- `anthropic/claude-opus-4.1`\n",
        "\n",
        "We will:\n",
        "- Do a quick primer on OpenRouter usage\n",
        "- Set up a lightweight comparison harness\n",
        "- Run demos: Q&A, summarization, information extraction, and coding help\n",
        "\n",
        "Prerequisites:\n",
        "- Get an OpenRouter API key and set environment variable `OPENROUTER_API_KEY`.\n",
        "- Install `openai` (the new unified client) for simple API calls.\n",
        "\n",
        "Note: Replace or parameterize models as availability/quotas may vary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Setup"
      ],
      "metadata": {
        "id": "fclaxsL9L9ZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "NtSKWciGH0vO"
      },
      "outputs": [],
      "source": [
        "#Installing Necessary libraries\n",
        "!pip install --quiet openai python-dotenv --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting Up Environment Varibles\n",
        "from google.colab import userdata\n",
        "OPENROUTER_API_KEY=userdata.get('OPENROUTER_API_KEY')\n"
      ],
      "metadata": {
        "id": "5ImulSfcIHry"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aheRUEcyH0vV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Use the official OpenAI python client pointed to OpenRouter.\n",
        "# Docs: https://openrouter.ai/docs\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        ")\n",
        "\n",
        "MODELS = [\n",
        "    \"google/gemini-2.5-pro\",\n",
        "    \"openai/gpt-5\",\n",
        "    \"anthropic/claude-opus-4.1\",\n",
        "]\n",
        "\n",
        "SYSTEM_PRIMER = (\n",
        "    \"You are a helpful, concise assistant. Prefer clear bullet points and short code.\"\n",
        ")\n",
        "\n",
        "def call_model(model: str, user_prompt: str, system_prompt: str = SYSTEM_PRIMER, **kwargs: Any) -> str:\n",
        "    \"\"\"Call a single model with a chat completion and return its text output.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        **kwargs,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def compare_models(user_prompt: str, system_prompt: str = SYSTEM_PRIMER, **kwargs: Any) -> Dict[str, str]:\n",
        "    \"\"\"Run the same prompt across all models and return a mapping of model->output.\"\"\"\n",
        "    outputs: Dict[str, str] = {}\n",
        "    for model in MODELS:\n",
        "        try:\n",
        "            outputs[model] = call_model(model, user_prompt, system_prompt, **kwargs)\n",
        "        except Exception as e:\n",
        "            outputs[model] = f\"<error: {type(e).__name__}: {e}>\"\n",
        "    return outputs\n",
        "\n",
        "def display_side_by_side(results: Dict[str, str]):\n",
        "    \"\"\"Nicely print results for quick visual comparison.\"\"\"\n",
        "    for model, text in results.items():\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"Model: {model}\")\n",
        "        print(\"-\" * 80)\n",
        "        print(text)\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7bPKvYgH0vY"
      },
      "source": [
        "## Quick OpenRouter usage primer\n",
        "\n",
        "OpenRouter is an API gateway for many frontier models under one endpoint.\n",
        "\n",
        "Basic call structure using the unified `openai` client:\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=OPENROUTER_API_KEY)\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"openai/gpt-5\",\n",
        "    messages=[{\"role\":\"user\",\"content\":\"Say hi in one sentence\"}]\n",
        ")\n",
        "print(resp.choices[0].message.content)\n",
        "```\n",
        "\n",
        "Weâ€™ll reuse this structure for all models by switching the `model` name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MV8JFgmH0va",
        "outputId": "c26d3d5f-6936-4c27-e692-9e9aae3759a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Model: google/gemini-2.5-pro\n",
            "--------------------------------------------------------------------------------\n",
            "Of course. Here is the difference between concurrency and parallelism.\n",
            "\n",
            "### Key Difference\n",
            "\n",
            "*   **Concurrency:** Is about *dealing* with multiple tasks at once. This can be done on a single CPU core by switching between tasks (context switching). It's a way to structure a program.\n",
            "*   **Parallelism:** Is about *doing* multiple tasks at once. This requires multiple CPU cores to execute tasks simultaneously. It's a hardware-dependent execution model.\n",
            "\n",
            "> **Analogy:** A chef is cooking two dishes (**concurrency**). They chop vegetables for one, then stir the other, then go back to chopping. They are managing both tasks, but only doing one action at a time. If they get a second chef to help, one chef can make one dish while the other makes the second dish. That is **parallelism**.\n",
            "\n",
            "### Short Example (Python)\n",
            "\n",
            "This example shows how Python's `threading` is used for concurrency and `multiprocessing` for parallelism.\n",
            "\n",
            "```python\n",
            "import threading\n",
            "import multiprocessing\n",
            "import time\n",
            "\n",
            "# A simple task that takes one second to run\n",
            "def do_work():\n",
            "    print(\"Working...\")\n",
            "    time.sleep(1)\n",
            "    print(\"...Done\")\n",
            "\n",
            "# --- Concurrency using Threads ---\n",
            "# Threads run interleaved, sharing one CPU core. Good for I/O-bound tasks.\n",
            "thread1 = threading.Thread(target=do_work)\n",
            "thread2 = threading.Thread(target=do_work)\n",
            "thread1.start()\n",
            "thread2.start()\n",
            "thread1.join()\n",
            "thread2.join()\n",
            "\n",
            "# --- Parallelism using Processes ---\n",
            "# Processes run on separate CPU cores, executing truly at the same time.\n",
            "if __name__ == \"__main__\":\n",
            "    process1 = multiprocessing.Process(target=do_work)\n",
            "    process2 = multiprocessing.Process(target=do_work)\n",
            "    process1.start()\n",
            "    process2.start()\n",
            "    process1.join()\n",
            "    process2.join()\n",
            "```\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Model: openai/gpt-5\n",
            "--------------------------------------------------------------------------------\n",
            "- Concurrency: Structuring a program to handle multiple tasks by making progress on them one at a time (they can be interleaved). Itâ€™s about dealing with many things at once.\n",
            "- Parallelism: Actually running multiple tasks at the same instant on different CPUs/cores. Itâ€™s about doing many things at once.\n",
            "\n",
            "Short example:\n",
            "- Cooking dinner:\n",
            "  - Concurrency: You chop vegetables, then stir the soup, then check the oven, switching between tasks so all progress.\n",
            "  - Parallelism: You chop vegetables while a friend stirs the soup at the same time on another stove burner.\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Model: anthropic/claude-opus-4.1\n",
            "--------------------------------------------------------------------------------\n",
            "## Key Differences\n",
            "\n",
            "**Concurrency:**\n",
            "- Multiple tasks **progress** at overlapping time periods\n",
            "- Tasks may not execute simultaneously \n",
            "- About **dealing with** multiple things at once\n",
            "- Can happen on a single CPU core (via context switching)\n",
            "\n",
            "**Parallelism:**\n",
            "- Multiple tasks **execute** at the exact same time\n",
            "- Tasks run simultaneously\n",
            "- About **doing** multiple things at once  \n",
            "- Requires multiple CPU cores\n",
            "\n",
            "## Example\n",
            "\n",
            "```python\n",
            "import time\n",
            "import asyncio\n",
            "from concurrent.futures import ProcessPoolExecutor\n",
            "\n",
            "# CONCURRENCY - Tasks interleave but don't run simultaneously\n",
            "async def concurrent_tasks():\n",
            "    async def task(name):\n",
            "        print(f\"Start {name}\")\n",
            "        await asyncio.sleep(1)  # Yields control to other tasks\n",
            "        print(f\"End {name}\")\n",
            "    \n",
            "    # Both tasks progress but take turns using the CPU\n",
            "    await asyncio.gather(task(\"A\"), task(\"B\"))\n",
            "\n",
            "# PARALLELISM - Tasks truly run at the same time\n",
            "def parallel_tasks():\n",
            "    def cpu_task(n):\n",
            "        return sum(i*i for i in range(n))\n",
            "    \n",
            "    with ProcessPoolExecutor(max_workers=2) as executor:\n",
            "        # Both calculations happen simultaneously on different cores\n",
            "        results = executor.map(cpu_task, [1000000, 1000000])\n",
            "```\n",
            "\n",
            "**Analogy:**\n",
            "- **Concurrency**: One person juggling multiple balls (switching attention between them)\n",
            "- **Parallelism**: Multiple people each juggling their own ball simultaneously\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 1: General Q&A\n",
        "prompt = \"Explain the difference between concurrency and parallelism with a short example.\"\n",
        "results = compare_models(prompt, temperature=0.2)\n",
        "display_side_by_side(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3AvaLIaH0vc",
        "outputId": "c8141158-5fb2-47fd-8e0b-5bf607e5dc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Model: google/gemini-2.5-pro\n",
            "--------------------------------------------------------------------------------\n",
            "Here is a summary of OpenRouter:\n",
            "\n",
            "*   **Unified Access:** Provides a single API to access many different AI models like GPT, Claude, and Gemini.\n",
            "*   **Easy Comparison:** Allows you to easily switch between models to test and compare their speed, style, and quality.\n",
            "*   **Faster Iteration:** Simplifies experimentation, helping you find the best model for your needs more quickly.\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Model: openai/gpt-5\n",
            "--------------------------------------------------------------------------------\n",
            "- Single API to access multiple models (e.g., GPT, Claude, Gemini) through one client/endpoint.\n",
            "- Easily switch model names to compare speed, style, and quality across providers.\n",
            "- Enables faster iteration and systematic, side-by-side evaluation.\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Model: anthropic/claude-opus-4.1\n",
            "--------------------------------------------------------------------------------\n",
            "â€¢ **Single API for multiple models**: OpenRouter offers unified access to various AI models (GPT, Claude, Gemini) through one API endpoint\n",
            "\n",
            "â€¢ **Easy experimentation**: Switch between models by simply changing the `model` parameter to compare speed, style, and quality\n",
            "\n",
            "â€¢ **Faster development**: Eliminates need for multiple API integrations, enabling rapid iteration and systematic model evaluation\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 2: Summarization\n",
        "text = (\n",
        "    \"\"\"\n",
        "OpenRouter provides access to many models like GPT, Claude, and Gemini via a single API.\n",
        "It simplifies experimenting across providers and comparing outputs. With one client and\n",
        "endpoint, you can switch `model` names to test speed, style, and quality differences.\n",
        "This enables faster iteration and systematic evaluation.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "prompt = f\"Summarize the following in 2-3 bullet points:\\n\\n{text}\"\n",
        "results = compare_models(prompt, temperature=0.2)\n",
        "display_side_by_side(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9yhM3cGH0ve",
        "outputId": "f7a08988-ba84-45a4-e7c1-1385bd081c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Model: google/gemini-2.5-pro\n",
            "--------------------------------------------------------------------------------\n",
            "```json\n",
            "{\n",
            "  \"order_id\": \"A219\",\n",
            "  \"street\": \"221B Baker Street\",\n",
            "  \"city\": \"London\",\n",
            "  \"date\": \"2025-02-10\",\n",
            "  \"total_amount\": 149.99,\n",
            "  \"currency\": \"USD\"\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Model: openai/gpt-5\n",
            "--------------------------------------------------------------------------------\n",
            "{\n",
            "  \"order_id\": \"A219\",\n",
            "  \"street\": \"221B Baker Street\",\n",
            "  \"city\": \"London\",\n",
            "  \"date\": \"2025-02-10\",\n",
            "  \"total_amount\": 149.99,\n",
            "  \"currency\": \"USD\"\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Model: anthropic/claude-opus-4.1\n",
            "--------------------------------------------------------------------------------\n",
            "```json\n",
            "{\n",
            "  \"order_id\": \"A219\",\n",
            "  \"street\": \"221B Baker Street\",\n",
            "  \"city\": \"London\",\n",
            "  \"date\": \"2025-02-10\",\n",
            "  \"total_amount\": 149.99,\n",
            "  \"currency\": \"USD\"\n",
            "}\n",
            "```\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 3: Information extraction\n",
        "snippet = \"Order #A219 ships to 221B Baker Street, London on 2025-02-10. Total: $149.99 USD.\"\n",
        "\n",
        "prompt = (\n",
        "    \"Extract a JSON object with keys: order_id, street, city, date, total_amount, currency. \"\n",
        "    f\"Text: {snippet}\"\n",
        ")\n",
        "results = compare_models(prompt, temperature=0)\n",
        "display_side_by_side(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "movIOfeaH0vg",
        "outputId": "b76529f3-ecf3-4934-85b5-1635074a03d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Model: google/gemini-2.5-pro\n",
            "--------------------------------------------------------------------------------\n",
            "Of course. Here is a Python function for FizzBuzz.\n",
            "\n",
            "This function iterates from 1 to `n` and uses a series of `if/elif/else` checks to build the list.\n",
            "\n",
            "```python\n",
            "def fizz_buzz(n):\n",
            "  \"\"\"\n",
            "  Generates a FizzBuzz list up to n.\n",
            "  \"\"\"\n",
            "  result = []\n",
            "  for i in range(1, n + 1):\n",
            "    if i % 15 == 0:\n",
            "      result.append('FizzBuzz')\n",
            "    elif i % 3 == 0:\n",
            "      result.append('Fizz')\n",
            "    elif i % 5 == 0:\n",
            "      result.append('Buzz')\n",
            "    else:\n",
            "      result.append(str(i))\n",
            "  return result\n",
            "\n",
            "# Example usage:\n",
            "print(fizz_buzz(15))\n",
            "# Output:\n",
            "# ['1', '2', 'Fizz', '4', 'Buzz', 'Fizz', '7', '8', 'Fizz', 'Buzz', '11', 'Fizz', '13', '14', 'FizzBuzz']\n",
            "```\n",
            "\n",
            "### Logic:\n",
            "*   Loop from `1` to `n`.\n",
            "*   First, check for the most specific condition: divisibility by 15 (`FizzBuzz`).\n",
            "*   Then, check for divisibility by 3 (`Fizz`) and 5 (`Buzz`).\n",
            "*   If none of the above, append the number as a string.\n",
            "\n",
            "A more concise version can be written using a list comprehension, though it can be less readable for beginners.\n",
            "\n",
            "```python\n",
            "def fizz_buzz_oneline(n):\n",
            "  return ['Fizz' * (i % 3 == 0) + 'Buzz' * (i % 5 == 0) or str(i) for i in range(1, n + 1)]\n",
            "```\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Model: openai/gpt-5\n",
            "--------------------------------------------------------------------------------\n",
            "def fizz_buzz(n):\n",
            "    \"\"\"Return list of strings from 1..n with Fizz/Buzz/FizzBuzz rules.\"\"\"\n",
            "    out = []\n",
            "    for i in range(1, n + 1):\n",
            "        s = ''\n",
            "        if i % 3 == 0:\n",
            "            s += 'Fizz'\n",
            "        if i % 5 == 0:\n",
            "            s += 'Buzz'\n",
            "        out.append(s or str(i))\n",
            "    return out\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Model: anthropic/claude-opus-4.1\n",
            "--------------------------------------------------------------------------------\n",
            "Here's a clean implementation of the FizzBuzz function:\n",
            "\n",
            "```python\n",
            "def fizz_buzz(n):\n",
            "    result = []\n",
            "    for i in range(1, n + 1):\n",
            "        if i % 15 == 0:\n",
            "            result.append('FizzBuzz')\n",
            "        elif i % 3 == 0:\n",
            "            result.append('Fizz')\n",
            "        elif i % 5 == 0:\n",
            "            result.append('Buzz')\n",
            "        else:\n",
            "            result.append(str(i))\n",
            "    return result\n",
            "```\n",
            "\n",
            "**Alternative concise version:**\n",
            "\n",
            "```python\n",
            "def fizz_buzz(n):\n",
            "    return ['FizzBuzz' if i % 15 == 0 else \n",
            "            'Fizz' if i % 3 == 0 else \n",
            "            'Buzz' if i % 5 == 0 else \n",
            "            str(i) \n",
            "            for i in range(1, n + 1)]\n",
            "```\n",
            "\n",
            "**Example usage:**\n",
            "```python\n",
            "print(fizz_buzz(15))\n",
            "# Output: ['1', '2', 'Fizz', '4', 'Buzz', 'Fizz', '7', '8', 'Fizz', 'Buzz', '11', 'Fizz', '13', '14', 'FizzBuzz']\n",
            "```\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 4: Coding assistance\n",
        "prompt = (\n",
        "    \"Write a Python function `fizz_buzz(n)` that returns a list of strings from 1..n \"\n",
        "    \"with 'Fizz' for multiples of 3, 'Buzz' for multiples of 5, and 'FizzBuzz' for multiples of both.\"\n",
        ")\n",
        "results = compare_models(prompt, temperature=0)\n",
        "display_side_by_side(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLtJZiumH0vi"
      },
      "source": [
        "### Notes and tips\n",
        "\n",
        "- Ensure `OPENROUTER_API_KEY` is set. Using a `.env` file is supported via `python-dotenv`.\n",
        "- Some models require provider-specific routing or may be rate-limited.\n",
        "- For reproducibility, keep `temperature` low for deterministic tasks like extraction.\n",
        "- Extend `MODELS` with other candidates from the Text Arena leaderboard.\n",
        "- Consider adding simple cost/latency tracking if you need deeper evaluation.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}